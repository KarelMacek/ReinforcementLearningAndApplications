{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy Prediction with Approximation\n",
    "\n",
    "What if\n",
    "\n",
    "- states are infititely many\n",
    "- states are finitely, but still too many\n",
    "\n",
    "$$\\hat v(s,w) \\approx v_{\\pi}(s)$$\n",
    "\n",
    "for $w\\in\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value-function Approximation\n",
    "\n",
    "In tabular methods, we did update the target\n",
    "$$\n",
    "s\\mapsto u\n",
    "$$\n",
    "where $u$ is the target, it can be:\n",
    "\n",
    "* $G_t$ for Monte Carlo\n",
    "* $R_{t+1}+\\gamma \\hat v(S_{t+1,w})$\n",
    "* $\\mathbb{E}_\\pi[R_{t+1}+\\gamma \\hat v(S_{t+1,w})|S_t=s]$\n",
    "\n",
    "Instead of tabular representation, we can update the approximation.\n",
    "\n",
    "Any supervised learning can be considered.\n",
    "\n",
    "Not any is suitable; we prefer:\n",
    "\n",
    "* Online methods - can be updated after each transition; can react to changes (including GPI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Prediction Objective $\\bar{VE}$\n",
    "\n",
    "Considering state weighting $\\mu(s)$ where $\\mu(s)\\geq 0$ and $\\sum_{s}\\mu(s)=1$.\n",
    "\n",
    "Using this, we can define Mean Squared Value Error\n",
    "\n",
    "$$\n",
    "\\bar{VE}(w) = \\sum_s \\mu(s)(v_{\\pi}(s)-\\hat v (s,w))^2\n",
    "$$\n",
    "\n",
    "For on-policy training, we call $\\mu(s)$ on-policy distribution.\n",
    "\n",
    "In continuing tasks, it is the *stationary* distribution under $\\pi$.\n",
    "\n",
    "For episodic tasks, it is more tricky:\n",
    "$$\\eta(s) = h(s) + \\sum_{\\bar s}\\eta(\\bar s)\\sum_a \\pi(a,\\bar s)p(s|\\bar{s},a)$$\n",
    "\n",
    "$$\\mu(s) = \\frac{\\eta(s)}{\\sum_s \\eta(s)}$$\n",
    "\n",
    "Question:\n",
    "\n",
    "* Is the prediction objective $\\bar{VE}$ the ultimate goal of our learning?\n",
    "\n",
    "However, let's consider global or local optimum of $\\bar{VE}$. In some cases it does not converge for RL.\n",
    "\n",
    "Question:\n",
    "\n",
    "* Why? Provide some intuition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic-gradient and Semi-gradient Methods\n",
    "\n",
    "Stochastic gradient descent (SGD):\n",
    "\n",
    "$$w_{t+1} = w_{t} - \\frac{1}{2}\\alpha\\nabla[v_{\\pi}(S_t)-\\hat{v}(S_t,w)]^2$$\n",
    "$$\n",
    "= w_{t} +\\alpha[v_{\\pi}(S_t)-\\hat{v}(S_t,w)]\\nabla\\hat{v}(S_t,w)\n",
    "$$\n",
    "Question:\n",
    "\n",
    "* Why do we call this *stochastic*?\n",
    "\n",
    "If we don't know $v_{\\pi}(S_t)$, we approximate it by target $U_t$\n",
    "\n",
    "$$\n",
    "w_{t+1}= w_{t} +\\alpha[U_t-\\hat{v}(S_t,w)]\\nabla\\hat{v}(S_t,w)\n",
    "$$\n",
    "\n",
    "For Monte Carlo $U_t$ is unbiased.\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-fe510bbb6ce95ccfec286aa98373fa99_hd.png\" width=\"65%\"/>\n",
    "\n",
    "For DP and TD - we bootstrap: $U_t$ depends on the last value of $w_t$. Thus it is *biased*.\n",
    "\n",
    "Then we speak about *semigradient* methods. They:\n",
    "\n",
    "* Have less theoretical guarantees.\n",
    "* Practically, converges faster (advantages of bootstrapping).\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQBKLq3I_pUmk-LLGTDQKQtDZ41XbTFhHSY3tOcFhCXwyn_YCkw\" width=\"65%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Methods\n",
    "<img src=\"https://i.stack.imgur.com/h68dd.png\" width=\"65%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for Linear Methods\n",
    "\n",
    "* Polynomial\n",
    "\n",
    "* Fourier basis\n",
    "\n",
    "* Coarse coding\n",
    "\n",
    "* Tile coding (symetrical vs. asymetrical offsets)\n",
    "\n",
    "* Radial basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Function Approximation\n",
    "\n",
    "Neural Networks\n",
    "\n",
    "Kernel Based (SVM)\n",
    "\n",
    "Lazy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Obligatory:\n",
    "\n",
    "* Consider a policy \"do nothing\" and approximate the value function for Cart Pole example in OpenAI Gym. E.g. Monte Carlo, e.g. linear with some coding.\n",
    "\n",
    "Optional:\n",
    "\n",
    "*  Same as above, for alternative settings of $\\alpha$. When does it \"works\"? When is it \"strange\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
