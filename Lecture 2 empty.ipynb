{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/82/Las_Vegas_slot_machines.jpg\" />\n",
    "\n",
    "Source: https://upload.wikimedia.org/wikipedia/commons/8/82/Las_Vegas_slot_machines.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the other words, we have only one state $s\\in S$. \n",
    "\n",
    "**Question**: Can we change the state by taking actions?\n",
    "\n",
    "Some notes on multi-armed bandit problem (source: <a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit#Empirical_motivation\">wikipedia</a>):\n",
    "\n",
    "- Applications e.g. in clinical trials. Each patient is an experiment, the arms are alternative treatments\n",
    "- Well known to be intrackable (no closed form solution): Originally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.\n",
    "- The version of the problem now commonly analyzed was formulated by Herbert Robbins in 1952.\n",
    "\n",
    "**Question**: Is it realy so difficult? Consider 2 actions, return as conditional Bernouli distribution, and 2 steps. If you are done, try 3 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1000 trials, we have a good understanding which arm is good and which is bad.\n",
    "\n",
    "Since we are interested in s\n",
    "$q(a) = \\mathbb{E}(R|A=a)$, we can approximate from the data set as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our completely random selection, the total reward is after 1000 of iterations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the possibilities to do it smarter than randomly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$\n",
    "\n",
    "$=\\frac{1}{n}\\left( X_n + \\sum_{i=1}^{n-1}X_{i} \\right)$\n",
    "\n",
    "$=\\frac{1}{n}\\left( X_n + (n-1)\\frac{1}{n-1}\\sum_{i=1}^{n-1}X_{i} \\right)$\n",
    "\n",
    "$=\\frac{1}{n}\\left( X_n + (n-1)\\bar{X}_{n-1} \\right)$\n",
    "\n",
    "$=\\frac{1}{n}\\left( X_n + n\\bar{X}_{n-1}-\\bar{X}_{n-1} \\right)$\n",
    "\n",
    "$=\\bar{X}_{n-1}+\\frac{1}{n}\\left( X_n -\\bar{X}_{n-1} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this formula, we will update the estimates of $Q$ for all bandits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Greedy Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "* What is the disadvantage of the greedy approach?\n",
    "* What ideas can be adopted to cope with that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "* If we have two actions (arms) and the learned Q is correct. What is the probability of selecting the right action if $\\epsilon=0.5$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timed Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound (UCB)\n",
    "Chernoff-Hoeffding bound:\n",
    "$$P(|\\bar{X}-\\mu|\\geq\\epsilon)\\leq 2 \\exp(-2\\epsilon^2N)$$\n",
    "Can be translated into an update algorithm:\n",
    "$$X_{ucb,j}=\\bar{X}_j + \\sqrt{2\\frac{\\ln{N}}{N_j}}$$\n",
    "\n",
    "## Non Stationary Bandits\n",
    "\n",
    "If statatistics of the process vary over time (mean, variance).\n",
    "\n",
    "Trick:\n",
    "$Q_n=Q_{n-1}+\\alpha(r_n+Q_{n-1})$\n",
    "\n",
    "$\\alpha$ instead of $1/N$\n",
    "\n",
    "**Questions**\n",
    "\n",
    "* Why it is good?\n",
    "* Why it is bad?\n",
    "\n",
    "## Contextual Bandits\n",
    "\n",
    "The state can change, but we cannot change the state by our actions.\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "* What can be a real-world example for this?\n",
    "* How to cope with that if the number of states is finite?\n",
    "* How to cope with that if the number of states is $\\mathbb{R}^n$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "* Obligatory: implement UCB agent and test it\n",
    "\n",
    "* Voluntary: find and simulate a realistic non-stationary process that can be subject to bandits.\n",
    "\n",
    "# Next Lecture: Dynamic Programming\n",
    "* How to make optimal decisions in *known* dynamic environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
