{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Methods\n",
    "\n",
    "Combining DP and MC:\n",
    "\n",
    "- Similarly to MC: model not needed, learning from *experience*\n",
    "- Updating the state before reaching the end of the epoch (*bootstrapping*)\n",
    "\n",
    "Variants of TD will be discussed not just during this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Prediction\n",
    "\n",
    "Given a policy $\\pi$, we want to quantify $v_\\pi(s)$ for all states.\n",
    "\n",
    "Simple every-visit MC calculates:\n",
    "\n",
    "$$\n",
    "V(S_t)\\gets V(S_t)+alpha[G_t-V(S_t)]\n",
    "$$\n",
    "Let's call this *constant-$\\alpha$ MC* - normally $\\alpha=\\frac{1}{N_{S_t}}$ where $N_{S_t}$ is the number of updates of $S_t$.\n",
    "\n",
    "Monte Carlo has to know $G_t$ that is known in the end of the episode.\n",
    "\n",
    "TD approach leverages bootstrapping instead:\n",
    "\n",
    "$V(S_t)\\gets V(S_t) + \\alpha[R_{t+1}+\\gamma V(S_{t+1})-V(S_t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN LAKE ALGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD prediction can be easily integrated with Policy Improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of TD Prediction Methods\n",
    "\n",
    "- Model not needed\n",
    "- Fully online (also continuous, not only episodic tasks)\n",
    "- Convergence? Yes.\n",
    "\n",
    "Better than MC - hard to say in general; in practice TD is more succesful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality of TD(0) \n",
    "\n",
    "Let's have finite amount of experience, e.g. 10 episodes.\n",
    "\n",
    "Basic incremental approach: to keep updating based on the experience until convergence achieved.\n",
    "\n",
    "We can apply:\n",
    "\n",
    "- constant-$\\alpha$ MC\n",
    "- TD(0)\n",
    "\n",
    "Who will be better?\n",
    "\n",
    "Constant-$\\alpha$ MC: *optimal* in terms of minimization of mean-squared-error from the *actual* returns $G$ in the training set.\n",
    "\n",
    "TD faster. Why?\n",
    "\n",
    "Better estimates used: not just *actual*, but *certainty-equivalence estimate* -  the models are coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa: On-policy TD Control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to use Generalized Policy Improvement.\n",
    "\n",
    "Similarly to Monte Carlo Methods, we distinguish:\n",
    "* on-policy\n",
    "* off-policy\n",
    "\n",
    "We want to estimate $q_\\pi(s,a)$. TD rule for this is:\n",
    "\n",
    "$$\n",
    "Q(S_t,A_t)\\gets Q(S_t,A_t) + \\alpha[R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "Thus, we consider for one update the tuple $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$ which gave the name to this approach: *Sarsa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN LAKE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa *converges* with probability 1 to an optimal policy as long as \n",
    "\n",
    "- all state-action pairs are visited an infinite number of times \n",
    "- and the policy in limit to the greedy policy (can be assured e.g. by $\\epsilon=1/t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning: Off-policy TD Control\n",
    "\n",
    "As we know, off-policy means that we distinguish target and behavior policy.\n",
    "\n",
    "The update is the following:\n",
    "\n",
    "$$\n",
    "Q(S_t,A_t)\\gets Q(S_t,A_t)  + \\alpha [R_{t+1}+\\gamma \\max_a Q(S_{t+1},a)-Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "where the behavior is driven by a policy that can be derived from $Q$, e.g. $\\epsilon$-greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `LAKE` not found.\n"
     ]
    }
   ],
   "source": [
    "FROZEN LAKE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN LAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(S_t,A_t)\\gets Q(S_t,A_t)  + \\alpha [R_{t+1}+\\gamma \\mathbb{E}_{\\pi} (Q(S_{t+1},A_{t+1})|S_t)-Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gets Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma\\sum_a \\pi(a|S_{t+1})Q(S_{t+1,a})-Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "It eliminates the variance due to random selection of $A_{t+1}$ in Sarsa.\n",
    "\n",
    "Expected Sarsa can be both on-policy and off-policy.\n",
    "\n",
    "What if target $\\pi$ cannot be randomized, but is fully greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximization Bias and Double Learning\n",
    "\n",
    "If $q(s,a)$ are all zeros, but the reward is random then, because of randomness: we will get one value as positive and it will be considered as maximum.\n",
    "\n",
    "Maximization bias:\n",
    "\n",
    "* If we use maximum of estimates as maximum of true values.\n",
    "\n",
    "Trick: double learning - we distinuish $Q_1$ and $Q_2$ and we alter them randomly:\n",
    "\n",
    "$$\n",
    "Q_1(S_t,A_t)\\gets Q(S_t,A_t)  + \\alpha [R_{t+1}+\\gamma Q_2(\\arg\\max_a Q_1(S_{t+1},a))-Q(S_t,A_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games, Afterstates, and Other Special Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After state = situation in environment immediately after an action is taken.\n",
    "\n",
    "In games, we know how the board will look like after our action.\n",
    "\n",
    "There are many action-state pairs that lead to the same afterstate. From value perspective, they are the same!\n",
    "\n",
    "Similar setup - in other deterministic and structured problems:\n",
    "\n",
    "- Queue in IT infrastructure\n",
    "- Deletion of a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* Online\n",
    "* Model free\n",
    "\n",
    "We learned:\n",
    "\n",
    "* Sarsa\n",
    "* Q-learning\n",
    "* Expected Sarsa\n",
    "\n",
    "Our TD is:\n",
    "\n",
    "* One-step = we can enhance to multi-step\n",
    "* Tabular = we can use approximations for $Q$\n",
    "* Model-free = we can do TD and benefit from models\n",
    "\n",
    "Note: TD without actions can be helpful in some dynamic systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
