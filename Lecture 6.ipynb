{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Methods\n",
    "\n",
    "Combining DP and MC:\n",
    "\n",
    "- Similarly to MC: model not needed, learning from *experience*\n",
    "- Updating the state before reaching the end of the epoch (*bootstrapping*)\n",
    "\n",
    "Variants of TD will be discussed not just during this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Prediction\n",
    "\n",
    "Given a policy $\\pi$, we want to quantify $v_\\pi(s)$ for all states.\n",
    "\n",
    "Simple every-visit MC calculates:\n",
    "\n",
    "$$\n",
    "V(S_t)\\gets V(S_t)+alpha[G_t-V(S_t)]\n",
    "$$\n",
    "Let's call this *constant-$\\alpha$ MC* - normally $\\alpha=\\frac{1}{N_{S_t}}$ where $N_{S_t}$ is the number of updates of $S_t$.\n",
    "\n",
    "Monte Carlo has to know $G_t$ that is known in the end of the episode.\n",
    "\n",
    "TD approach leverages bootstrapping instead:\n",
    "\n",
    "$V(S_t)\\gets V(S_t) + \\alpha[R_{t+1}+\\gamma V(S_{t+1})-V(S_t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "V = np.zeros(64)\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "for i_episode in range(20000):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        old_observation = observation\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        V[old_observation]+=alpha*(reward+gamma*V[observation]-V[old_observation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake = V.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20b2bc4a208>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD prediction can be easily integrated with Policy Improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of TD Prediction Methods\n",
    "\n",
    "- Model not needed\n",
    "- Fully online (also continuous, not only episodic tasks)\n",
    "- Convergence? Yes.\n",
    "\n",
    "Better than MC - hard to say in general; in practice TD is more succesful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality of TD(0) \n",
    "\n",
    "Let's have finite amount of experience, e.g. 10 episodes.\n",
    "\n",
    "Basic incremental approach: to keep updating based on the experience until convergence achieved.\n",
    "\n",
    "We can apply:\n",
    "\n",
    "- constant-$\\alpha$ MC\n",
    "- TD(0)\n",
    "\n",
    "Who will be better?\n",
    "\n",
    "Constant-$\\alpha$ MC: *optimal* in terms of minimization of mean-squared-error from the *actual* returns $G$ in the training set.\n",
    "\n",
    "TD faster. Why?\n",
    "\n",
    "Better estimates used: not just *actual*, but *certainty-equivalence estimate* -  the models are coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa: On-policy TD Control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to use Generalized Policy Improvement.\n",
    "\n",
    "Similarly to Monte Carlo Methods, we distinguish:\n",
    "* on-policy\n",
    "* off-policy\n",
    "\n",
    "We want to estimate $q_\\pi(s,a)$. TD rule for this is:\n",
    "\n",
    "$$\n",
    "Q(S_t,A_t)\\gets Q(S_t,A_t) + \\alpha[R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "Thus, we consider for one update the tuple $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$ which gave the name to this approach: *Sarsa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN LAKE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa *converges* with probability 1 to an optimal policy as long as \n",
    "\n",
    "- all state-action pairs are visited an infinite number of times \n",
    "- and the policy in limit to the greedy policy (can be assured e.g. by $\\epsilon=1/t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning: Off-policy TD Control\n",
    "\n",
    "As we know, off-policy means that we distinguish target and behavior policy.\n",
    "\n",
    "The update is the following:\n",
    "\n",
    "$$\n",
    "Q(S_t,A_t)\\gets Q(S_t,A_t)  + \\alpha [R_{t+1}+\\gamma \\max_a Q(S_{t+1},a)-Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "where the behavior is driven by a policy that can be derived from $Q$, e.g. $\\epsilon$-greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `LAKE` not found.\n"
     ]
    }
   ],
   "source": [
    "FROZEN LAKE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN LAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(S_t,A_t)\\gets Q(S_t,A_t)  + \\alpha [R_{t+1}+\\gamma \\mathbb{E}_{\\pi} (Q(S_{t+1},A_{t+1})|S_t)-Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gets Q(S_t,A_t) + \\alpha [R_{t+1} + \\gamma\\sum_a \\pi(a|S_{t+1})Q(S_{t+1,a})-Q(S_t,A_t)]\n",
    "$$\n",
    "\n",
    "It eliminates the variance due to random selection of $A_{t+1}$ in Sarsa.\n",
    "\n",
    "Expected Sarsa can be both on-policy and off-policy.\n",
    "\n",
    "What if target $\\pi$ cannot be randomized, but is fully greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximization Bias and Double Learning\n",
    "\n",
    "If $q(s,a)$ are all zeros, but the reward is random then, because of randomness: we will get one value as positive and it will be considered as maximum.\n",
    "\n",
    "Maximization bias:\n",
    "\n",
    "* If we use maximum of estimates as maximum of true values.\n",
    "\n",
    "Trick: double learning - we distinuish $Q_1$ and $Q_2$ and we alter them randomly:\n",
    "\n",
    "$$\n",
    "Q_1(S_t,A_t)\\gets Q(S_t,A_t)  + \\alpha [R_{t+1}+\\gamma Q_2(\\arg\\max_a Q_1(S_{t+1},a))-Q(S_t,A_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Games, Afterstates, and Other Special Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After state = situation in environment immediately after an action is taken.\n",
    "\n",
    "In games, we know how the board will look like after our action.\n",
    "\n",
    "There are many action-state pairs that lead to the same afterstate. From value perspective, they are the same!\n",
    "\n",
    "Similar setup - in other deterministic and structured problems:\n",
    "\n",
    "- Queue in IT infrastructure\n",
    "- Deletion of a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* Online\n",
    "* Model free\n",
    "\n",
    "We learned:\n",
    "\n",
    "* Sarsa\n",
    "* Q-learning\n",
    "* Expected Sarsa\n",
    "\n",
    "Our TD is:\n",
    "\n",
    "* One-step = we can enhance to multi-step\n",
    "* Tabular = we can use approximations for $Q$\n",
    "* Model-free = we can do TD and benefit from models\n",
    "\n",
    "Note: TD without actions can be helpful in some dynamic systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
