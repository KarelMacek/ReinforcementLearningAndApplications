{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning and Learning with Tabular Methods\n",
    "\n",
    "We distinguish:\n",
    "- model-based - DP\n",
    "- model-free - MC, TD\n",
    "\n",
    "## Models and Planning\n",
    "\n",
    "We distinguish:\n",
    "- sample models => what is *an* episode?\n",
    "- distribution models => what are *all* episodes?\n",
    "\n",
    "Random sample one-step tabular Q-planning\n",
    "\n",
    "Why an unified view is needed:\n",
    "* Using same machinery for updates.\n",
    "* Possibility of incremental planning - can be interrupted at any time.\n",
    "\n",
    "## Dyna: Integrating Planning, Acting, and Learning\n",
    "\n",
    "Option 1:\n",
    "* Acting => Direct RL\n",
    "\n",
    "Option 2:\n",
    "* Acting => Model learning => Planning \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ZZSpd50mGNbEXg8HsoVFSA.png\" width=\"50%\" />\n",
    "\n",
    "Question:\n",
    "\n",
    "* What the search control can mean?\n",
    "\n",
    "Dyna-QT (deterministic)\n",
    "<img src=\"https://images.slideplayer.com/27/9056261/slides/slide_9.jpg\" width=\"80%\"/>\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Good utilization of the computing time in the background.\n",
    "- Same update mechanism for two different sources of experience (both simulated and real)\n",
    "\n",
    "## When the Model is Wrong\n",
    "\n",
    "Possible reasons:\n",
    "\n",
    "* Stochasticity\n",
    "* Changes of environment (non-sationary)\n",
    "* Approximations (including discretization)\n",
    "\n",
    "Implications:\n",
    "* Suboptimality\n",
    "\n",
    "In some cases:\n",
    "* Suboptimal policy by planning\n",
    "* Suboptimal decisions in real experience \n",
    "* Correction by the real experience\n",
    "* Optimal policy by planning\n",
    "\n",
    "This tend to happen if the model is *optimistic*.\n",
    "\n",
    "Question:\n",
    "* What will happen if the environment becomes better (i.e. providing higher rewards than before somewhere)?\n",
    "\n",
    "DynaQ+\n",
    "* To use $r+\\kappa\\sqrt{\\tau}$ instead of $r$ only.\n",
    "\n",
    "## Prioritized Sweeping\n",
    "\n",
    "Dyna selects the state and action for the update uniformly.\n",
    "\n",
    "No reason to update zeros by zeros.\n",
    "\n",
    "More experience, more reasonable state-action pairs, but still not all.\n",
    "\n",
    "Idea (backward focusing):\n",
    "* Go backward from the end state.\n",
    "* We speak about reward, not about end state=>go backward from states where the value changed\n",
    "\n",
    "Extra idea (prioritized sweeping):\n",
    "* Focus on states where the change was big.\n",
    "\n",
    "<img src=\"http://www.incompleteideas.net/book/ebook/pseudotmp19.png\" />\n",
    "\n",
    "Extension to stochastic:\n",
    "* Model maintains the counts.\n",
    "* Applying expected update.\n",
    "\n",
    "\n",
    "## Expected vs. Sample Updates\n",
    "\n",
    "Expected updates:\n",
    "* Use the full information.\n",
    "\n",
    "Sample updates:\n",
    "* Can be faster - especially if expected samples not possible.\n",
    "* Estimation from estimates is more efficient (we know better what is in the end before notifying all).\n",
    "\n",
    "## Trajectory Sampling\n",
    "\n",
    "Which states to update in the Dyna algorithm?\n",
    "\n",
    "Possibility to sample & update the whole trajectory, e.g. generated by the policy of the interest (on-policy).\n",
    "\n",
    "# Homework\n",
    "\n",
    "Obligatory:\n",
    "* Implement Dyna-Q for predictive maintenance problem.\n",
    "\n",
    "Optional:\n",
    "* Implement also Dyna-Q+ and test it with a sudden change in the middle of the training (e.g. significantly lower probability of failre). Will Dyna-Q+ bring some value?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
