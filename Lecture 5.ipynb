{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Gym - Frozen Lake\n",
    "\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "\n",
    "https://gym.openai.com/docs/\n",
    "\n",
    "It provides various environments, including video games and control problems. Let's start with toy examples\n",
    "\n",
    "https://gym.openai.com/envs/#toy_text\n",
    "\n",
    "in particular with Frozen Lake problem\n",
    "\n",
    "https://gym.openai.com/envs/FrozenLake8x8-v0/\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-P2GC1uKB-ss/UqHSAXOIXhI/AAAAAAAAAMU/JFNXwAFmV1c/s1600/800px-Frozen_Lake_-_Kosovo.JPG\" />\n",
    "\n",
    "Let's see how it is implemented in Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        clear_output()\n",
    "        env.render()\n",
    "        time.sleep(0.999)\n",
    "        #print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four possible actions - directions where we try to move.\n",
    "\n",
    "\"Try\" means ice is slippery and our step might result in a different state than expected. We don't know exactly how. **Welcome to Reinforcement Learning!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation does not tell us which direction is north and which direction is south. We have to figure it out during the interaction with the environment. **Welcome to Reinforcement Learning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of states is $8\\times 8$, i.e. $64$. They correspond to the positions on the lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SFFFFFFF\n",
    "    FFFFFFFF\n",
    "    FFFHFFFF\n",
    "    FFFFFHFF\n",
    "    FFFHFFFF\n",
    "    FHHFFFHF\n",
    "    FHFFHFHF\n",
    "    FFFHFFFG\n",
    "    \n",
    "Where \n",
    "\n",
    "- `S` is start\n",
    "- `F` is frozen lake -  you can move there\n",
    "- `H` is hole - you will fall there and end the episode\n",
    "- `G` is the place which you want to reach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this environment, we want to find the optimal policy $\\pi^{*}$ so we can safely reach the goal without falling into a hole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Monte Carlo Methods\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Monaco_Monte_Carlo_1.jpg/1200px-Monaco_Monte_Carlo_1.jpg\" />\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Monte_Carlo\">Monte Carlo</a>, part of Monacco known as a place of cassionos, gave the name to <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_method\">methods based on random sampling</a>.\n",
    "\n",
    "In Monte Carlo, we assume that we don't have the model $p(s',r|s,a)$ of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Monte Carlo Prediction\n",
    "\n",
    "We are given a policy $\\pi$, we want to know $v_{\\pi}(s)$ for all $s$.\n",
    "\n",
    "It is suitable for *episodic* tasks only. The whole idea is that for given policy $\\pi$ we:\n",
    "\n",
    "* Sample an episode \n",
    "* For each state $s$ visited during the episode, we quantify the return $G$\n",
    "* We calculate the value function for that state as $v_\\pi(s)$ of all returns that relate to $s$ from all already sampled episodes\n",
    "* This process is iterated\n",
    "\n",
    "The backup diagram looks like this:\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/MC5.png\" width=\"50%\"/>\n",
    "\n",
    "**Question**:\n",
    "\n",
    "* Do we need *bootstrapping*, i.e. $v_\\pi(s')$ to update $v_\\pi(s)$? <a href=\"https://datascience.stackexchange.com/questions/26938/what-exactly-is-bootstrapping-in-reinforcement-learning\">Hint<a/>.\n",
    "\n",
    "Update strategies:\n",
    "\n",
    "* First-visit - for a state, we consider only its first visit in the episode\n",
    "* Every-visit - for a state, we consider all visits in the episode\n",
    "\n",
    "### Example\n",
    "Let's implement the algorithm for Frozen Lake. First, we have to define $\\gamma$. Note that $\\gamma$ is not part of the environment.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "* Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a policy. Let's keep it simple for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pi(s):\n",
    "    return np.random.randint(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-11-02 04:56:17,894] Making new env: FrozenLake8x8-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.003716225850810045, 1: 0.0054641323289057984, 2: 0.005623687092938593, 3: 0.007611951468950891, 4: 0.014299666409243813, 5: 0.025236619093151107, 6: 0.04494507446456874, 7: 0.034153000196943964, 8: 0.004468929987904574, 9: 0.0072372157661682816, 10: 0.004416078505074799, 11: 0.006401682323738126, 12: 0.014912218676564195, 13: 0.020759443305580022, 14: 0.03678730683357182, 15: 0.033045262556350714, 16: 0.004400435245163378, 17: 0.004702209999103089, 18: 0.0007064126900516699, 19: 0, 20: 0.012233680523468961, 21: 0.02151240696090669, 22: 0.048426209515567996, 23: 0.05907103878743424, 24: 0.00648399563168673, 25: 0.0046670045794323526, 26: 0.002694458376934921, 27: 0.006530433257746656, 28: 0.00936777407669713, 29: 0, 30: 0.06854653585616957, 31: 0.09889376604180707, 32: 0.006211352564805115, 33: 0.0030274178754337157, 34: 0.0006817123099151571, 35: 0, 36: 0.004930526945899872, 37: 0.00629123405072781, 38: 0.07158463777403194, 39: 0.2724575445330732, 40: 0.0, 41: 0, 42: 0, 43: 0.0, 44: 0.014806387224924538, 45: 0.01986041930069958, 46: 0, 47: 0.653739177331842, 48: 0.0, 49: 0, 50: 0.0, 51: 0.0, 52: 0, 53: 0.1476936083856864, 54: 0, 55: 1.5162397200170918, 56: 0.0, 57: 0.0, 58: 0.0, 59: 0, 60: 0.0, 61: 0.9475364475343666, 62: 0.2, 63: 0}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "V = {i:0 for i in range(64)}\n",
    "N = {i:0 for i in range(64)}\n",
    "for i_episode in range(20000):\n",
    "    observation = env.reset()\n",
    "    state_reward_pairs = []\n",
    "    for t in range(100):\n",
    "        action = pi(observation)\n",
    "        observation_old = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        state_reward_pairs.append((observation_old,reward))\n",
    "        \n",
    "        if done:\n",
    "#             print(done)\n",
    "            break\n",
    "    G = 0\n",
    "    for pair in state_reward_pairs[::-1]:\n",
    "        state, reward = pair\n",
    "        G = reward + gamma*G\n",
    "        N[state] += 1\n",
    "        V[state] += G/N[state]\n",
    "\n",
    "print(V)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Estimation of Action Values\n",
    "\n",
    "In this case, we want to estimate the action-value function $q_\\pi (s,a)$ for given policy $\\pi$.\n",
    "\n",
    "**Question**\n",
    "\n",
    "* Why is this more important than in case of Dynamic Programming?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "Q = {i:{j:0 for j in range(4)} for i in range(64)}\n",
    "N = {i:{j:0 for j in range(4)} for i in range(64)}\n",
    "for i_episode in range(20000):\n",
    "    observation = env.reset()\n",
    "    state_action_reward_tuples = []\n",
    "    for t in range(100):\n",
    "        action = pi(observation)\n",
    "        observation_old = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        state_action_reward_tuples.append((observation_old,action,reward))\n",
    "        \n",
    "        if done:\n",
    "#             print(done)\n",
    "            break\n",
    "    G = 0\n",
    "    for my_tuple in state_action_reward_tuples[::-1]:\n",
    "        state, action, reward = my_tuple\n",
    "        G = reward + gamma*G\n",
    "        N[state][action] += 1\n",
    "        Q[state][action] += G/N[state][action]\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluation can assure convergence if each action-state pair is selected *asymptotically many times*. This makes the situation similar to *bandits*.\n",
    "\n",
    "We will see that this needs with Monte Carlo *special care* to be assured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Control\n",
    "\n",
    "Similarly to DP, we can iterate *policy evaluation* and *policy improvement*. If the *policy evaluation* is based on MC approach, we speak about *MC policy iteration*.\n",
    "\n",
    "We will use the principle of *generalized policy iteration* as we will be selective regarding the states that are being updated. \n",
    "\n",
    "We assume that the evaluation of $q_{\\pi_k}$ is perfect which is assured if:\n",
    "\n",
    "* Infinite number of episodes\n",
    "* Each episode starts with random pair $(s,a)$\n",
    "\n",
    "Then, the policy improvement is much simpler than in case of DP:\n",
    "\n",
    "$$\n",
    "\\pi_{k+1}(s) = \\arg\\max_{a} q_{\\pi_k}(s,a)\n",
    "$$\n",
    "\n",
    "Will the policy improvement work?\n",
    "\n",
    "$$\n",
    "q_{\\pi_k}(s,\\pi_{k+1}(s)) = q_{\\pi_k}(s,\\arg\\max_a q_{\\pi_k}(s,a))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\max_a q_{\\pi_k}(s,a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\geq q_{\\pi_k}(s,\\pi_k(s))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= v_{\\pi_k}(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditions for the evaluation are too strict and practically not useful. Two approaches how to cope with that:\n",
    "\n",
    "- To keep evaluating $q_{\\pi_k}$ until the changes are small.\n",
    "- After one episode, do directly the policy improvement step.\n",
    "\n",
    "Let's see the second option in detail.\n",
    "\n",
    "### Monte Carlo Exploring Starts\n",
    "\n",
    "At first, we have to be sure that we can start from any $(s,a)$.\n",
    "\n",
    "This is somehow arfificial, we have to make the environment to be in arbitrary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9b6d545b4306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.env.s = 1\n",
    "observation,reward,_,_ = env.step(3)\n",
    "[observation,reward]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = {i:{j:0 for j in range(4)} for i in range(64)}\n",
    "N = {i:{j:0 for j in range(4)} for i in range(64)}\n",
    "pi = {i:0 for i in range(64)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20000):\n",
    "    s = np.random.randint(0,64)\n",
    "    a = np.random.randint(0,4)\n",
    "    env.reset()\n",
    "    env.env.s = s\n",
    "    observation,reward,done,info = env.step(a)\n",
    "    state_action_reward_tuples = [(s,a,reward)]\n",
    "    t = 0\n",
    "    while not done:\n",
    "        action = pi[observation]\n",
    "        observation_old = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        state_action_reward_tuples.append((observation_old,action,reward))\n",
    "        t=+1\n",
    "        if done or t==99:\n",
    "            break\n",
    "        \n",
    "    G = 0\n",
    "    visited_states = []\n",
    "    for my_tuple in state_action_reward_tuples[::-1]:\n",
    "        state, action, reward = my_tuple\n",
    "        visited_states.append(state)\n",
    "        G = reward + gamma*G\n",
    "        N[state][action] += 1\n",
    "        Q[state][action] += G/N[state][action]\n",
    "    \n",
    "    for state in list(set(visited_states)):\n",
    "        best_value = -10000000000\n",
    "        best_action = None\n",
    "        for a in range(4):\n",
    "            if Q[state][a]>best_value:\n",
    "                best_value = Q[state][a]\n",
    "                best_action = a\n",
    "        pi[state] = best_action\n",
    "    print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Control without Exploring States (On Policy)\n",
    "\n",
    "Exploring initial states - not very realistic. Even with `gym` we had to use less standard API `env.env.s` to change the initial state. In some systems, this will not be possible.\n",
    "\n",
    "Another approach is to ensure that all actions will be selected infinitely often. There are two ways how to cope with that:\n",
    "\n",
    "* On-policy - we improve the policy that is used for exploration\n",
    "* Off-policy - the exploration has own policy; we update another one\n",
    "\n",
    "For the on-policy approach, we assume that the policy is *soft*, i.e. all actions are possible $\\pi(a|s)>0$ for all $s$ and $a$. We can adopt $\\epsilon$-greedy that we know from the world of bandits.\n",
    "\n",
    "That is:\n",
    "$$\\pi(a|s)=\\frac{\\epsilon}{|\\mathcal{A}(s)|}$$\n",
    "for all actions with the exception of the greedy action (greedy in terms of $q$):\n",
    "$$\n",
    "\\pi(a|s) = 1-\\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s)|}\n",
    "$$\n",
    "\n",
    "We will need a way how to sample actions proportionally randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(list(range(3)),p=[0.5,0.4,0.1],size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will result in a slightly different initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.8\n",
    "Q = {i:{j:0 for j in range(4)} for i in range(64)}\n",
    "N = {i:{j:0 for j in range(4)} for i in range(64)}\n",
    "pi = np.ones([64,4])/4\n",
    "def sample_action(policy):\n",
    "    return np.random.choice(list(range(len(policy))),p=policy)\n",
    "sample_action(pi[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20000):\n",
    "    observation = env.reset()\n",
    "    state_action_reward_tuples = []\n",
    "    t = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = sample_action(pi[observation,:])\n",
    "        observation_old = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        state_action_reward_tuples.append((observation_old,action,reward))\n",
    "        t=+1\n",
    "        if done or t==99:\n",
    "            break\n",
    "        \n",
    "    G = 0\n",
    "    visited_states = []\n",
    "    for my_tuple in state_action_reward_tuples[::-1]:\n",
    "        state, action, reward = my_tuple\n",
    "        visited_states.append(state)\n",
    "        G = reward + gamma*G\n",
    "        N[state][action] += 1\n",
    "        Q[state][action] += G/N[state][action]\n",
    "    \n",
    "    for state in list(set(visited_states)):\n",
    "        best_value = -10000000000\n",
    "        best_action = None\n",
    "        for a in range(4):\n",
    "            if Q[state][a]>best_value:\n",
    "                best_value = Q[state][a]\n",
    "                best_action = a\n",
    "        \n",
    "        for action in range(4):\n",
    "            pi[state,action] = epsilon/4\n",
    "        pi[state,best_action] = 1 - epsilon + epsilon/4\n",
    "        \n",
    "    print(pi[visited_states,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "q_\\pi(s,\\pi'(s)) = \\sum_a \\pi'(a|s) q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\\epsilon}{|\\mathcal{A}(s)|} \\sum_a q_\\pi(s,a) + (1-\\epsilon)\\max_a q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\geq \\frac{\\epsilon}{|\\mathcal{A}(s)|} \\sum_a q_\\pi(s,a) + (1-\\epsilon)\\max_a q_\\pi(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy Prediction via Importance Sampling\n",
    "\n",
    "* We want to learn the optimal, i.e. *target* policy $\\pi$ - responsible for exploitation\n",
    "* We interact with the environment based on *behavior* policy $b$ - responsible for explorations\n",
    "\n",
    "We say that $b$ has coverage of $\\pi$ iff $\\pi(a|s)>0$ implies $b(a|s)>0$.\n",
    "\n",
    "We want calculate $q$ from the results based on behavior policy. The trick is *weight average* and the weights express the match with the estimation policy.\n",
    "\n",
    "\n",
    "We can decomposeeach of them by <a href=\"https://en.wikipedia.org/wiki/Chain_rule\">chain rule</a> and based on Markov property like this:\n",
    "$$\n",
    "\\prod_{k=1}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|s_k,a_k)\n",
    "$$\n",
    "\n",
    "Given a trajectory, we can calculate its relative probability under target and estimation policies\n",
    "\n",
    "$$\\rho_{t:T-1} = \n",
    "\\frac{\n",
    "\\prod_{k=1}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|s_k,a_k)\n",
    "}{\n",
    "\\prod_{k=1}^{T-1}b(A_k|S_k)p(S_{k+1}|s_k,a_k)\n",
    "}$$\n",
    "we can reduce the fraction and get\n",
    "$$\n",
    "=  \n",
    "\\frac{\n",
    "\\prod_{k=1}^{T-1}\\pi(A_k|S_k)\n",
    "}{\n",
    "\\prod_{k=1}^{T-1}b(A_k|S_k)\n",
    "}$$\n",
    "which does not depent on model $p$ (otherwise, we could not speak about Monte Carlo method).\n",
    "\n",
    "We distinguish *ordinary* importance sampling\n",
    "$$\n",
    "V(s) = \\frac{\\sum_{t \\in \\mathcal{T}} \\rho_{t:T(t-1)}G_t}{|\\mathcal{T}(s)|}\n",
    "$$\n",
    "\n",
    "and *weighted* importance sampling\n",
    "$$\n",
    "V(s) = \\frac{\\sum_{t \\in \\mathcal{T}} \\rho_{t:T(t-1)}G_t}{\\sum_{t \\in \\mathcal{T}} \\rho_{t:T(t-1)}}\n",
    "$$\n",
    "\n",
    "Note: even if we have the notation here in terms of $V$, we can similarly do it for $Q$. The only difference is that we have less letters in the notation.\n",
    "\n",
    "**Question:**\n",
    "\n",
    "- What is the difference between these two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Implementation\n",
    "\n",
    "* For ordinary importance sampling: no problem, we simply calculate the weighted $G_t$ and then update it recursively (increasing the nominator by 1)\n",
    "* For weighted importance sampling: something else is needed (similar principles will be followed):\n",
    "\n",
    "Let's have some returns for one state $G_1,G_2,\\dots G_{n-1}$  and the corresponding importance weights $W_1,W_2,\\dots W_{n-1}$. We want to estimate:\n",
    "\n",
    "$$\n",
    "V_n = \\frac{\\sum_{k=1}^{n-1} W_k G_k}{\\sum_{k=1}^{n-1} W_k}\n",
    "$$\n",
    "\n",
    "similarly to other incremental implementations, we will maintain the nominator as a cummulative sum $C_n = \\sum_{k=1}^{n-1} W_k$. Then we can do the incremental update like this:\n",
    "$$\n",
    "V_{n+1} = V_n + \\frac{W_n}{C_n}\\left[G_n-V_n\\right]\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "C_{n+1} = C_n + W_{n+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy Monte Carlo Control\n",
    "### Estimation of $Q$ for $\\pi$\n",
    "\n",
    "- Input: policy $\\pi$\n",
    "- Initialize $Q$ arbitrarily and $C(s,a)\\gets=0$\n",
    "- In a loop\n",
    " - take a behavior policy $b$ with coverage of $\\pi$\n",
    " - generate an episode using $b$: $S_0,A_0,R_1\\dots A_{T-1},R_{T},S_{T}$\n",
    " - initiate $G\\gets 0$ and $W\\gets 1$\n",
    " - for $t=T-1,T-2,\\dots,0$:\n",
    "  - update \n",
    "   - $G\\gets \\gamma G + R_{t+1}$\n",
    "   - $C(S_t,A_t)\\gets C(S_t,A_t) + W$\n",
    "   - $Q(S_t,A_t)\\gets Q(S_t,A_t) + \\frac{W}{C(S_t,A_t)} [G - Q(S_t,A_t)]$\n",
    "   - $W\\gets W \\cdot \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$\n",
    "  - if $W=0$ exit for loop\n",
    "  \n",
    "**Question**:\n",
    "\n",
    "- Is this *first* visit or *every* visit update?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy Monte Carlo Control $\\pi\\approx \\pi^{*}$\n",
    "\n",
    "- Input: policy $\\pi$\n",
    "- Initialize $Q$ arbitrarily and $C(s,a)\\gets=0$\n",
    "- $\\pi(s)\\gets\\arg\\max_a Q(s,a)$\n",
    "- In a loop\n",
    " - take a behavior policy $b$ with coverage of $\\pi$\n",
    " - generate an episode using $b$: $S_0,A_0,R_1\\dots A_{T-1},R_{T},S_{T}$\n",
    " - initiate $G\\gets 0$ and $W\\gets 1$\n",
    " - for $t=T-1,T-2,\\dots,0$:\n",
    "  - update \n",
    "   - $G\\gets \\gamma G + R_{t+1}$\n",
    "   - $C(S_t,A_t)\\gets C(S_t,A_t) + W$\n",
    "   - $Q(S_t,A_t)\\gets Q(S_t,A_t) + \\frac{W}{C(S_t,A_t)} [G - Q(S_t,A_t)]$\n",
    "   - $\\pi(S_t)\\gets\\arg\\max_a Q(S_t,a)$\n",
    "   - if $A_t\\neq \\pi(S_t)$ then exit for loop\n",
    "   - $W\\gets W \\cdot \\frac{1}{b(A_t|S_t)}$\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Monte Carlo has several advantages over Dynamic Programming:\n",
    "-  Learns directly from interaction with environment\n",
    " - Full models not needed\n",
    " - No need to learn about *all* states\n",
    " - Less harm by Markovian violations \n",
    "- MC methods provide an alternate policy evaluation process\n",
    "- Challenge to be addressed: maintaining sufficient exploration\n",
    " - Exploring starts, soft policies\n",
    "- No bootstrapping (as opposed to DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Obligatory\n",
    "\n",
    "- Implement the off-policy control with weighted importance sampling for Frozen Lake. As a behavior $b$ use random actions.\n",
    "\n",
    "Optional\n",
    "\n",
    "- Create the wrapper for your environment (one of the previous home works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
