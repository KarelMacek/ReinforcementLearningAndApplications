{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Maintenance Problem\n",
    "\n",
    "<img src=\"https://www.theholidayspot.com/christmas/images/symbols/chimney-sweep.jpg\"/>\n",
    "\n",
    "States: healthy, faulty $\\mathcal{S}=\\{0,1\\}$\n",
    "\n",
    "Actions: nothing, repair $\\mathcal{A}=\\{0,1\\}$\n",
    "\n",
    "If repair, then healthy, i.e.\n",
    "\n",
    "$p(r=-10,s'=0|s=\\forall,a=1)=1$\n",
    "\n",
    "If nothing done and faulty, then faulty, i.e.\n",
    "\n",
    "$p(r=-1,s'=1|s=1,a=0)=1$\n",
    "\n",
    "If nothing done and heathy, then may get faulty\n",
    "\n",
    "$\n",
    "p(r=0,s'=0|s=0,a=0)=\\alpha\n",
    "$\n",
    "\n",
    "$\n",
    "p(r=0,s'=1|s=0,a=0)=1-\\alpha\n",
    "$\n",
    "\n",
    "and we consider a general parameter $\\gamma$. More on predictive maintenance you can find <a href=\"https://www2.humusoft.cz/www/papers/tcp11/019_berka.pdf\"/>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = {}\n",
    "#p[s,a]={(r,s'):P(r,s'|s,a)}\n",
    "p[(0,1)]={(-10,0):1}\n",
    "p[(1,1)]={(-10,0):1}\n",
    "p[(1,0)]={(-1,1):1}\n",
    "p[(0,0)]={(0,0):0.95,(0,1):0.05}\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies and Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return means the long-term reward, we consider it in a discounted form:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\dots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$ \n",
    "\n",
    "Note that there is a relationship between $G_t$ and $G_{t+1}$\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} = R_{t+1}+\\sum_{k=1}^\\infty \\gamma^k R_{t+k+1}\n",
    "= R_{t+1}+\\sum_{k=0}^\\infty \\gamma^{k+1} R_{t+k+2} = R_{t+1}+\\gamma\\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} = R_{t+1} + \\gamma G_{t+1}\n",
    "$$\n",
    "We are interested in return $G_t$ for given $S_t$ when following a policy $\\pi$ which we denote for all $s$ by **state-value function for policy $\\pi$**:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s]\n",
    "$$\n",
    "\n",
    "Similarly, we are interested in return for state $s$ and action $a$. This is denoted as **state-value function for policy $\\pi$**:\n",
    "$$\n",
    "q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a]\n",
    "$$\n",
    "There is an important recursive relation for $v_{\\pi}(s)$ (called **Bellman equation**):\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) && = && \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\\n",
    "&& = && \\mathbb{E}_{\\pi}[R_t+\\gamma G_{t+1}|S_t=s] \\\\\n",
    "&& = &&\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left( r+\\mathbb{E}_{\\pi}[\\gamma G_{t+1}|S_{t+1}=s']\\right) \\\\\n",
    "&& = &&\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left( r+\\gamma v_{\\pi}(s') \\right) \n",
    "\\end{align}\n",
    "$$\n",
    "Which can be represented by so called **backup diagram**\n",
    "<img src=\"http://www.incompleteideas.net/book/ebook/figtmp10.png\"/>\n",
    "Source <a href=\"http://www.incompleteideas.net/book/ebook/figtmp10.png\">http://www.incompleteideas.net/book/ebook/figtmp10.png</a>\n",
    "\n",
    "**Question**\n",
    "\n",
    "- Looking at the (b) part of diagram, what recursive relation does hold for action-state function $q_{\\pi}(s,a)$?\n",
    "- What is the value function for policy $\\pi(0)=0$ and $\\pi(1)=1$ in the optimal maintenance problem? Hint: Solve the Bellman equation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies and Value Functions\n",
    "Considering the set of all policies, we can define a <a href=\"https://en.wikipedia.org/wiki/Partially_ordered_set\">partial ordering</a> like this:\n",
    "$\\pi\\geq \\pi'$ if and only if $v_{\\pi}(s)\\geq v_{\\pi'}(s)$ for all $s$. \n",
    "\n",
    "There might be multiple optimal strategies. All of them share the same value function:\n",
    "$$\n",
    "v^{*}(s)=\\max_\\pi v_\\pi(s)\n",
    "$$\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "- Proof that the ordering is partial.\n",
    "- Provide an example of an MDP where more than one strategies are optimal.\n",
    "\n",
    "Similarly, we can define the optimal action-value function\n",
    "$$\n",
    "q^{*}(s,a)=\\max_{\\pi} q_{\\pi}(s,a)\n",
    "$$\n",
    "\n",
    "**Question**:\n",
    "\n",
    "- Do $q^{*}$ correspond to the same optimal policies as $v^{*}$?\n",
    "\n",
    "Hint: $q^{*}(s,a)=\\mathbb{E}[R_{t+1}+\\gamma v^{*}(S_{t+1})|S_t=s,A_t=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman optimality equation:\n",
    "$$\n",
    "\\begin{align}\n",
    "v^{*}(s) && = && \\max_a q_\\pi^{*} (s,a)\\\\\n",
    "&& = && \\max_a \\mathbb{E}_{\\pi^{*}}[G_t | S_t=s,A_t=a]\\\\\n",
    "&& = && \\max_a \\mathbb{E}_{\\pi^{*}}[R_{t+1} + \\gamma v^{*}(S_{t+1}) | S_t=s,A_t=a]\\\\\n",
    "&& = && \\max_a \\sum_{s',r}p(s',r|s,a)[r+\\gamma v^{*}(s')]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similarly:\n",
    "$$\n",
    "q^{*}(s,a) = \\sum_{s',r}p(s',r|s,a)[r+\\gamma\\max_{a'}q^{*}(s',a')]\n",
    "$$\n",
    "\n",
    "In this case, we have backup diagrams like this:\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMlEqT-T1HFB3THaiGWDCaIDkISr0dfp1GEzPVtgOmCaXno4wFWw\"/>\n",
    "Source <a href=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMlEqT-T1HFB3THaiGWDCaIDkISr0dfp1GEzPVtgOmCaXno4wFWw\">https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMlEqT-T1HFB3THaiGWDCaIDkISr0dfp1GEzPVtgOmCaXno4wFWw</a>\n",
    "\n",
    "These equations can be solved as system of nonlinear equations.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "- Why non-linear?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality and Approximations\n",
    "\n",
    "How to cope with high dimension of $\\mathcal{A}$ and $\\mathcal{S}$?\n",
    "\n",
    "Question:\n",
    "\n",
    "- In terms of memory?\n",
    "- In terms of states that are being updated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Work\n",
    "\n",
    "Obligatory:\n",
    "\n",
    "- Define own Markov Decision Process - all considered elements in the definition.\n",
    "\n",
    "Optional:\n",
    "\n",
    "- Solve the Bellman optimality equations and determine that gamma analytically. Send the solution 24 before the lecture (LaTeX or scanned documents).\n",
    "\n",
    "# Next Time - Dynamic Programming Methods\n",
    "\n",
    "- How to evaluate $v_\\pi$ iteratively.\n",
    "- How to use that information to improve the policy.\n",
    "- How to iterate these two steps.\n",
    "- What are the other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
