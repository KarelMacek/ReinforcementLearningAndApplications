{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Maintenance Problem\n",
    "\n",
    "<img src=\"https://www.theholidayspot.com/christmas/images/symbols/chimney-sweep.jpg\"/>\n",
    "\n",
    "States: healthy, faulty $\\mathcal{S}=\\{0,1\\}$\n",
    "\n",
    "Actions: nothing, repair $\\mathcal{A}=\\{0,1\\}$\n",
    "\n",
    "If repair, then healthy, i.e.\n",
    "\n",
    "$p(r'=-10,s'=0|s=\\forall,a=1)=1$\n",
    "\n",
    "If nothing done and faulty, then faulty, i.e.\n",
    "\n",
    "$p(r'=-1,s'=1|s=1,a=0)=1$\n",
    "\n",
    "If nothing done and heathy, then may get faulty\n",
    "\n",
    "$\n",
    "p(r'=0,s'=1|s=0,a=0)=\\alpha\n",
    "$\n",
    "\n",
    "$\n",
    "p(r'=0,s'=1|s=0,a=0)=1-\\alpha\n",
    "$\n",
    "\n",
    "and we consider a general parameter $\\gamma$. More on predictive maintenance you can find <a href=\"https://www2.humusoft.cz/www/papers/tcp11/019_berka.pdf\"/>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = {}\n",
    "#p[s,a]={(r,s'):P(r,s'|s,a)}\n",
    "p[(0,1)]={(-10,0):1}\n",
    "p[(1,1)]={(-10,0):1}\n",
    "p[(1,0)]={(-1,1):1}\n",
    "p[(0,0)]={(0,0):0.95,(0,1):0.05}\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies and Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return means the long-term reward, we consider it in a discounted form:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\dots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$ \n",
    "\n",
    "Note that there is a relationship between $G_t$ and $G_{t+1}$\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} = R_{t+1}+\\sum_{k=1}^\\infty \\gamma^k R_{t+k+1}\n",
    "= R_{t+1}+\\sum_{k=0}^\\infty \\gamma^{k+1} R_{t+k+2} = R_{t+1}+\\gamma\\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} = R_{t+1} + \\gamma G_{t+1}\n",
    "$$\n",
    "We are interested in return $G_t$ for given $S_t$ when following a policy $\\pi$ which we denote for all $s$ by **state-value function for policy $\\pi$**:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s]\n",
    "$$\n",
    "\n",
    "Similarly, we are interested in return for state $s$ and action $a$. This is denoted as **state-value function for policy $\\pi$**:\n",
    "$$\n",
    "q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a]\n",
    "$$\n",
    "There is an important recursive relation for $v_{\\pi}(s)$ (called **Bellman equation**):\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) && = && \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\\n",
    "&& = && \\mathbb{E}_{\\pi}[R_t+\\gamma G_{t+1}|S_t=s] \\\\\n",
    "&& = &&\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left( r+\\mathbb{E}_{\\pi}[\\gamma G_{t+1}|S_{t+1}=s']\\right) \\\\\n",
    "&& = &&\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left( r+v_{\\pi}(s') \\right) \n",
    "\\end{align}\n",
    "$$\n",
    "Which can be represented by so called **backup diagram**\n",
    "<img src=\"http://www.incompleteideas.net/book/ebook/figtmp10.png\"/>\n",
    "Source <a href=\"http://www.incompleteideas.net/book/ebook/figtmp10.png\">http://www.incompleteideas.net/book/ebook/figtmp10.png</a>\n",
    "\n",
    "**Question**\n",
    "\n",
    "- Looking at the (b) part of diagram, what recursive relation does hold for action-state function $q_{\\pi}(s,a)$?\n",
    "- What is the value function for policy $\\pi(0)=0$ and $\\pi(1)=1$ in the optimal maintenance problem? Hint: Solve the Bellman equation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies and Value Functions\n",
    "Considering the set of all policies, we can define a <a href=\"https://en.wikipedia.org/wiki/Partially_ordered_set\">partial ordering</a> like this:\n",
    "$\\pi\\geq \\pi'$ if and only if $v_{\\pi}(s)\\geq v_{\\pi'}(s)$ for all $s$. \n",
    "\n",
    "There might be multiple optimal strategies. All of them share the same value function:\n",
    "$$\n",
    "v^{*}(s)=\\max_\\pi v_\\pi(s)\n",
    "$$\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "- Proof that the ordering is partial.\n",
    "- Provide an example of an MDP where more than one strategies are optimal.\n",
    "\n",
    "Similarly, we can define the optimal action-value function\n",
    "$$\n",
    "q^{*}(s,a)=\\max_{\\pi} q_{\\pi}(s,a)\n",
    "$$\n",
    "\n",
    "**Question**:\n",
    "\n",
    "- Do $q^{*}$ correspond to the same optimal policies as $v^{*}$?\n",
    "\n",
    "Hint: $q^{*}(s,a)=\\mathbb{E}[R_{t+1}+\\gamma v^{*}(S_{t+1})|S_t=s,A_t=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman optimality equation:\n",
    "$$\n",
    "\\begin{align}\n",
    "v^{*}(s) && = && \\max_a q_\\pi^{*} (s,a)\\\\\n",
    "&& = && \\max_a \\mathbb{E}_{\\pi^{*}}[G_t | S_t=s,A_t=a]\\\\\n",
    "&& = && \\max_a \\mathbb{E}_{\\pi^{*}}[R_{t+1} + v^{*}(S_{t+1}) | S_t=s,A_t=a]\\\\\n",
    "&& = && \\max_a \\sum_{s',r}p(s',r|s,a)[r+v^{*}(s')]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similarly:\n",
    "$$\n",
    "q^{*}(s,a) = \\sum_{s',r}p(s',r|s,a)[r+\\max_{a'}q^{*}(s',a')]\n",
    "$$\n",
    "\n",
    "In this case, we have backup diagrams like this:\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMlEqT-T1HFB3THaiGWDCaIDkISr0dfp1GEzPVtgOmCaXno4wFWw\"/>\n",
    "Source <a href=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMlEqT-T1HFB3THaiGWDCaIDkISr0dfp1GEzPVtgOmCaXno4wFWw\">https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMlEqT-T1HFB3THaiGWDCaIDkISr0dfp1GEzPVtgOmCaXno4wFWw</a>\n",
    "\n",
    "These equations can be solved as system of nonlinear equations.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "- Why non-linear?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality and Approximations\n",
    "\n",
    "How to cope with high dimension of $\\mathcal{A}$ and $\\mathcal{S}$?\n",
    "\n",
    "Question:\n",
    "\n",
    "- In terms of memory?\n",
    "- In terms of states that are being updated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "In this case, we assume that we know the transition model $p(\\cdot,\\cdot|\\cdot,\\cdot)$ exactly.\n",
    "\n",
    "We will use it to calculate the optimal state-value function $v^{*}$ iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation \n",
    "$$v_{k+1}(s) = \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+v_{k}(s')]$$\n",
    "\n",
    "We can continue this recursion until $\\max_s|v_{k+1}(s)-v_{k}(s)|<\\theta$.\n",
    "\n",
    "Questions:\n",
    "\n",
    "- How to intiate $v_0(s)$?\n",
    "- What would $v_0(s)=0$ mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 1: -10.0}\n",
      "{0: -0.45, 1: -10.0}\n",
      "{0: -0.83475, 1: -10.405}\n",
      "{0: -1.1819362500000001, 1: -10.751275}\n",
      "{0: -1.4943628687500001, 1: -11.063742625}\n",
      "{0: -1.7755486709062502, 1: -11.344926581875}\n",
      "{0: -2.028615809809219, 1: -11.597993803815625}\n",
      "{0: -2.2563762385585853, 1: -11.825754228828297}\n",
      "{0: -2.461360624264864, 1: -12.030738614702727}\n",
      "{0: -2.6458465714080814, 1: -12.215224561838378}\n",
      "{0: -2.811883923836637, 1: -12.381261914267274}\n",
      "{0: -2.961317541022352, 1: -12.530695531452974}\n",
      "{0: -3.0958077964894946, 1: -12.665185786920116}\n",
      "{0: -3.2168490264099234, 1: -12.786227016840545}\n",
      "{0: -3.3257861333383087, 1: -12.89516412376893}\n",
      "{0: -3.423829529573856, 1: -12.993207520004479}\n",
      "{0: -3.512068586185848, 1: -13.08144657661647}\n",
      "{0: -3.5914837371366413, 1: -13.160861727567264}\n",
      "{0: -3.662957372992355, 1: -13.232335363422976}\n",
      "{0: -3.727283645262497, 1: -13.29666163569312}\n",
      "{0: -3.7851772903056253, 1: -13.354555280736248}\n",
      "{0: -3.8372815708444405, 1: -13.406659561275063}\n",
      "{0: -3.884175423329374, 1: -13.453553413759996}\n",
      "{0: -3.9263798905658147, 1: -13.495757880996436}\n",
      "{0: -3.9643639110786113, 1: -13.533741901509234}\n",
      "{0: -3.998549529540128, 1: -13.56792751997075}\n",
      "{0: -4.029316586155494, 1: -13.598694576586116}\n",
      "{0: -4.057006937109322, 1: -13.626384927539945}\n",
      "{0: -4.081928252967768, 1: -13.65130624339839}\n",
      "{0: -4.104357437240369, 1: -13.67373542767099}\n",
      "{0: -4.12454370308571, 1: -13.693921693516332}\n",
      "{0: -4.142711342346517, 1: -13.71208933277714}\n",
      "{0: -4.159062217681242, 1: -13.728440208111865}\n",
      "{0: -4.173778005482497, 1: -13.743155995913119}\n",
      "{0: -4.187022214503624, 1: -13.756400204934247}\n",
      "{0: -4.1989420026226405, 1: -13.768319993053263}\n",
      "{0: -4.209669811929754, 1: -13.779047802360378}\n",
      "{0: -4.219324840306157, 1: -13.78870283073678}\n",
      "{0: -4.22801436584492, 1: -13.797392356275541}\n",
      "{0: -4.2358349388298056, 1: -13.805212929260428}\n",
      "{0: -4.242873454516203, 1: -13.812251444946824}\n",
      "{0: -4.24920811863396, 1: -13.818586109064583}\n",
      "{0: -4.254909316339942, 1: -13.824287306770564}\n",
      "{0: -4.260040394275326, 1: -13.829418384705948}\n",
      "{0: -4.264658364417171, 1: -13.834036354847793}\n",
      "{0: -4.268814537544832, 1: -13.838192527975455}\n",
      "{0: -4.272555093359727, 1: -13.841933083790348}\n",
      "{0: -4.275921593593132, 1: -13.845299584023754}\n",
      "{0: -4.278951443803197, 1: -13.848329434233818}\n",
      "{0: -4.281678308992255, 1: -13.851056299422877}\n",
      "{0: -4.284132487662408, 1: -13.85351047809303}\n",
      "{0: -4.286341248465545, 1: -13.855719238896167}\n",
      "{0: -4.288329133188369, 1: -13.85770712361899}\n",
      "{0: -4.29011822943891, 1: -13.859496219869532}\n",
      "{0: -4.291728416064397, 1: -13.861106406495018}\n",
      "{0: -4.293177584027335, 1: -13.862555574457957}\n",
      "{0: -4.29448183519398, 1: -13.863859825624601}\n",
      "{0: -4.29565566124396, 1: -13.865033651674583}\n",
      "{0: -4.296712104688942, 1: -13.866090095119564}\n",
      "{0: -4.297662903789426, 1: -13.867040894220048}\n"
     ]
    }
   ],
   "source": [
    "pi = {0:0,1:1}\n",
    "import numpy as np\n",
    "def policy_evaluation(pi):\n",
    "    V = {}\n",
    "    V[0] = 0\n",
    "    V[1] = 0\n",
    "    difference_large = True\n",
    "    while difference_large:\n",
    "        V_old = V.copy()\n",
    "        difference_large = False\n",
    "        for s in range(2):\n",
    "            V[s] = 0\n",
    "            a = pi[s]\n",
    "            for key,val in p[(s,a)].items():\n",
    "                r = key[0]\n",
    "                s_next = key[1]\n",
    "                V[s]+= val*(r+gamma*V_old[s_next])\n",
    "            difference = np.abs(V[s]-V_old[s])\n",
    "            \n",
    "            difference_large = difference_large or difference>0.001\n",
    "        print(V)\n",
    "        \n",
    "    return V\n",
    "V = policy_evaluation(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "$q_\\pi(s,a) = \\mathbb{E}[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=a]$\n",
    "\n",
    "### Policy improvement theorem\n",
    "Let $\\pi$ and $\\pi'$ be two deterministic policies such that, for all $s$: \n",
    "$$\n",
    "q_{\\pi}(s,\\pi'(s))\\geq v_{\\pi}(s)\n",
    "$$\n",
    "then the policy $\\pi'$ must be at least as good as $\\pi$ or even better, i.e. for all $s$:\n",
    "$$\n",
    "v_{\\pi'}(s)\\geq v_{\\pi}(s)\n",
    "$$\n",
    "#### Proof\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) && \\leq && q_{\\pi}(s,\\pi'(s))\\\\\n",
    "&& = && \\mathbb{E}[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=\\pi'(s)]\\\\\n",
    "&& = && \\mathbb{E}_{\\pi'}[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s]\\\\\n",
    "&& \\leq && \\mathbb{E}_{\\pi'}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1},\\pi'(S_{t+1}))|S_t=s]\\\\\n",
    "&& = && \\mathbb{E}_{\\pi'}[R_{t+1}+\\gamma \\mathbb{E}_{\\pi'}[R_{t+2}+\\gamma v_\\pi(S_{t+2})|S_{t+1}=s]|S_t=s]\\\\\n",
    "&& \\vdots  && \\\\\n",
    "&& \\leq && \\mathbb{E}_{\\pi'}[R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4}\\dots| S_t=s] \\\\\n",
    "&& = && v_{\\pi'}(s)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Question**\n",
    "\n",
    "- If the inequality in the condition of the theorem strict at least in one cases, will it result into strict inequality in the implication of the theorem?\n",
    "\n",
    "\n",
    "We can apply the principle to all possible states and all possible actions if $v_{\\pi}$ is given:\n",
    "$$\n",
    "\\pi'(s) = \\arg\\max_a q_{\\pi}(s,a)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): -4.298518622979861, (0, 1): -13.867896613410483, (1, 0): -13.480336804798045, (1, 1): -13.867896613410483}\n",
      "{0: 0, 1: 0}\n"
     ]
    }
   ],
   "source": [
    "def policy_improvement(V):\n",
    "    q={}\n",
    "    pi_new = {}\n",
    "    for s in range(2):\n",
    "        best_a = -1\n",
    "        best_value = -10000000000\n",
    "        for a in range(2):\n",
    "            q[(s,a)] = 0\n",
    "            for key,probability in p[(s,a)].items():\n",
    "                r,s_next = key\n",
    "                q[(s,a)]+=probability*(r+gamma*V[s_next])\n",
    "            if q[(s,a)]>best_value:\n",
    "                best_a = a\n",
    "                best_value = q[(s,a)] \n",
    "        pi_new[s]= best_a\n",
    "    print(q)\n",
    "    print(pi_new)\n",
    "    return pi_new\n",
    "pi = policy_improvement(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "The process of policy evaluation and policy improvement can be iterated until stable (same policy before and after policy improvement)\n",
    "\n",
    "This is policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======0======\n",
      "{0: 0.0, 1: -1.0}\n",
      "{0: -0.045000000000000005, 1: -1.9}\n",
      "{0: -0.12397500000000002, 1: -2.71}\n",
      "{0: -0.22794862500000002, 1: -3.439}\n",
      "{0: -0.34965107437500004, 1: -4.0951}\n",
      "{0: -0.4832311685906251, 1: -4.68559}\n",
      "{0: -0.6240141991449845, 1: -5.217031}\n",
      "{0: -0.7682985352689617, 1: -5.6953279000000006}\n",
      "{0: -0.9131850031549622, 1: -6.12579511}\n",
      "{0: -1.0564339576474926, 1: -6.5132155990000005}\n",
      "{0: -1.1963457357436063, 1: -6.861894039100001}\n",
      "{0: -1.3316608358202833, 1: -7.175704635190001}\n",
      "{0: -1.4614767232098922, 1: -7.458134171671}\n",
      "{0: -1.585178636069653, 1: -7.7123207545039}\n",
      "{0: -1.7023821677922288, 1: -7.94108867905351}\n",
      "{0: -1.8128857440197637, 1: -8.14697981114816}\n",
      "{0: -1.9166314026385654, 1: -8.332281830033345}\n",
      "{0: -2.013672531607474, 1: -8.49905364703001}\n",
      "{0: -2.104147428640741, 1: -8.649148282327008}\n",
      "{0: -2.1882577241925487, 1: -8.784233454094307}\n",
      "{0: -2.266250859618873, 1: -8.905810108684877}\n",
      "{0: -2.338405939864956, 1: -9.01522909781639}\n",
      "{0: -2.405022387986275, 1: -9.113706188034751}\n",
      "{0: -2.466410920189829, 1: -9.202335569231277}\n",
      "{0: -2.5228864373777107, 1: -9.28210201230815}\n",
      "{0: -2.574762494511809, 1: -9.353891811077334}\n",
      "{0: -2.622347064306077, 1: -9.4185026299696}\n",
      "{0: -2.665939358330328, 1: -9.47665236697264}\n",
      "{0: -2.7058275078861995, 1: -9.528987130275377}\n",
      "{0: -2.7422869401050924, 1: -9.57608841724784}\n",
      "{0: -2.7755793125660073, 1: -9.618479575523056}\n",
      "{0: -2.8059518931424736, 1: -9.656631617970751}\n",
      "{0: -2.833637291445499, 1: -9.690968456173676}\n",
      "{0: -2.858853464713717, 1: -9.72187161055631}\n",
      "{0: -2.881803934805262, 1: -9.749684449500679}\n",
      "{0: -2.90267816448603, 1: -9.77471600455061}\n",
      "{0: -2.9216520508403327, 1: -9.797244404095549}\n",
      "{0: -2.938888501652784, 1: -9.817519963685994}\n",
      "{0: -2.954538067279, 1: -9.835767967317395}\n",
      "{0: -2.968739606052828, 1: -9.852191170585655}\n",
      "{0: -2.9816209658515227, 1: -9.86697205352709}\n",
      "{0: -2.9932996682117707, 1: -9.880274848174382}\n",
      "{0: -3.003883584488911, 1: -9.892247363356944}\n",
      "{0: -3.0134715960890817, 1: -9.90302262702125}\n",
      "{0: -3.022154232872121, 1: -9.912720364319124}\n",
      "{0: -3.0300142855000245, 1: -9.921448327887212}\n",
      "{0: -3.0371273888574457, 1: -9.929303495098491}\n",
      "{0: -3.0435625747525483, 1: -9.936373145588643}\n",
      "{0: -3.049382792964918, 1: -9.942735831029779}\n",
      "{0: -3.0546454003813457, 1: -9.948462247926802}\n",
      "{0: -3.059402618482757, 1: -9.953616023134122}\n",
      "{0: -3.0637019598437925, 1: -9.95825442082071}\n",
      "{0: -3.0675866246033747, 1: -9.96242897873864}\n",
      "{0: -3.071095868079124, 1: -9.966186080864777}\n",
      "{0: -3.074265340846566, 1: -9.9695674727783}\n",
      "{0: -3.0771274026988378, 1: -9.97261072550047}\n",
      "{0: -3.0797114119550275, 1: -9.975349652950424}\n",
      "{0: -3.082043991604318, 1: -9.977814687655382}\n",
      "{0: -3.084149273766184, 1: -9.980033218889844}\n",
      "{0: -3.08604912392013, 1: -9.98202989700086}\n",
      "{0: -3.0877633463167498, 1: -9.983826907300774}\n",
      "{0: -3.089309871929356, 1: -9.985444216570697}\n",
      "{0: -3.0907049302452805, 1: -9.986899794913628}\n",
      "{0: -3.091963206130828, 1: -9.988209815422264}\n",
      "{0: -3.09309798293586, 1: -9.989388833880039}\n",
      "{0: -3.0941212729347622, 1: -9.990449950492035}\n",
      "{0: -3.095043936131363, 1: -9.991404955442832}\n",
      "{(0, 0): -3.095875788387243, (0, 1): -12.785539542518226, (1, 0): -9.992264459898548, (1, 1): -12.785539542518226}\n",
      "{0: 0, 1: 0}\n"
     ]
    }
   ],
   "source": [
    "policy_changed = True\n",
    "i = 0\n",
    "while policy_changed:\n",
    "    print('======={}======'.format(i))\n",
    "    i+=1\n",
    "    V = policy_evaluation(pi)\n",
    "    pi_old = pi\n",
    "    pi = policy_improvement(V)\n",
    "    policy_changed = False\n",
    "    for s in range(2):\n",
    "        if pi[s]!=pi_old[s]:\n",
    "            policy_changed=True\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "Policy iteration: we run the whole policy evaluation before doing any improvement.\n",
    "\n",
    "Practically, we can update after each step of policy evaluation.\n",
    "$$\n",
    "v_{n+1}(s) = \\max_a \\left[ \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma v_n(s') \\right) \\right]\n",
    "$$\n",
    "\n",
    "The difference can be summarized as follows:\n",
    "<img src=\"https://i.stack.imgur.com/wGuj5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Dynamic Programming\n",
    "\n",
    "In both value iteration and policy iteration, we assume that the value is updated for all states $s$.\n",
    "\n",
    "We can call the update of all states a **sweep** - as we sweep the whole floor, we sweep the whole  $\\mathcal{S}$.\n",
    "<img src=\"https://c.pxhere.com/photos/4c/95/black_and_white_caretaker_cleaning_janitor_job_man_person_street-1057139.jpg!d\" />\n",
    "\n",
    "Backgamonn has approximately $10^{20}$ states. \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/63/Backgammon_board.jpg\" />\n",
    "\n",
    "Asynchronous Dynamic Programming:\n",
    "\n",
    "- Be selective where you update.\n",
    "- Guarantee that in the long-term you will visit everything invitite number times.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Focus on updating states where you are right now $\\mathcal{S}_{\\textrm{now}}\\subset \\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration\n",
    "\n",
    "Common aspects of policy iteration, value iteration, and asynchronous dynamic programming:\n",
    "\n",
    "- estimation of value function\n",
    "- greedy actions with respect to the value function of the next state\n",
    "\n",
    "We can hardly find a Reinforcement Learning algorithm that does not have these two aspects.\n",
    "\n",
    "<img src=\"http://incompleteideas.net/book/ebook/figtmp18.png\" width=\"25%\" />\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://incompleteideas.net/book/ebook/imgtmp5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency of Dynamic Programming\n",
    "\n",
    "- Polynomial complexity in number of states and number of actions.\n",
    "- Usually working better than worst-case theoretical bounds.\n",
    "- If we know the model, Dynamic Programming is a approach to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Work\n",
    "\n",
    "Obligatory:\n",
    "\n",
    "- Implement the value iteration and test for different values of gamma. For what gamma, it is negligible whether to repair or do noting? Send the solution 24 before the lecture (python code or Jupyter notebook).\n",
    "\n",
    "Optional:\n",
    "\n",
    "- Solve the Bellman optimality equations and determine that gamma analytically. Send the solution 24 before the lecture (LaTeX or scanned documents).\n",
    "\n",
    "# Next Time - Monte Carlo Methods\n",
    "\n",
    "- Model is not known.\n",
    "- However, we can interact with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
