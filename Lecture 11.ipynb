{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy Methods with Approximation\n",
    "\n",
    "Let's remind what we should already know. What is on-policy approach; what is off-policy?\n",
    "\n",
    "* SARSA\n",
    "$$\n",
    "Q(S_t,A_t) \\gets R_{t+1} +\\gamma Q(S_{t+1},A_{t+1}) \n",
    "$$\n",
    "* Q Learning\n",
    "$$\n",
    "Q(S_t,A_t) \\gets R_{t+1} +\\gamma \\max_{a'}Q(S_{t+1},a') \n",
    "$$\n",
    "\n",
    "Question:\n",
    "\n",
    "* What about DP?\n",
    "\n",
    "Main benefits of off-policy methods:\n",
    "\n",
    "* Not mixing exploration and exploitation.\n",
    "* Can use recorded experience of a different agent (even human).\n",
    "\n",
    "Disclaimer:\n",
    "\n",
    "* This lecture is quite closed to the cutting edge of the technologies.\n",
    "* It attempts to provide some intuition and understanding.\n",
    "\n",
    "Two main challenges:\n",
    "\n",
    "* Target of the update ($U_t$ is not the $U_t$ of our interest) => can be solved by importance sampling\n",
    "* Distribution of the updates => to be addressed here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-gradient Methods\n",
    "\n",
    "Semi-gradient off-policy TD(0):\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t + \\alpha \\rho_t \\delta_t \\nabla \\hat v (S_t,w_t)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\rho_t = \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\delta_t = R_{t+1}+\\gamma \\hat v (S_{t+1},w_t) - \\hat v (S_t,w_t) \n",
    "$$\n",
    "\n",
    "Possible variants:\n",
    "* Expected Sarsa\n",
    "* $n$-step tree-backup algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Off-policy divergence\n",
    "<img src=\"images/11ExamplesOfOffPolicyDivergence.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deadly Triad\n",
    "\n",
    "We need:\n",
    "\n",
    "* Function approximation - if the number of states is large or even not finite.\n",
    "* Bootstrapping - if we cannot wait till the end of episodes\n",
    "* Off-policy training - especially when learning multiple targets (people and animals learn many things, towards general AI)\n",
    "\n",
    "If all three considered, the learning tends to be unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Value-function Geometry\n",
    "<img src=\"images/11LinearValueFunctionGeometry.png\" />\n",
    "$$\n",
    "||v||^2_{\\mu}=\\sum_s \\mu(s)v(s)^2\n",
    "$$\n",
    "Using this notation we have \n",
    "$$\n",
    "\\overline{VE}(w) = ||v_w -v_\\pi||_{\\mu}^{2}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\overline{BE}(w) = ||B_\\pi v_w - v_w ||_{\\mu}^{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overline{PBE}(w) = ||\\Pi(B_\\pi v_w - v_w) ||_{\\mu}^{2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent in the Bellman Error\n",
    "\n",
    "Problems:\n",
    "\n",
    "- slow\n",
    "- wrong value functions\n",
    "- simply a bad objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bellman Error is Not Learnable\n",
    "\n",
    "Let's consider these 2 Markov Reward Processes (one action only).\n",
    "<img src=\"images/11TheBellmanErrorIsNotLearnable1.png\"/>\n",
    "They produce identical sequence of rewards. Internally, they are different. By change, we represent the value function by the same $w$.\n",
    "\n",
    "For $\\gamma=0$, the true values are 1, 0, and 2 respectively. $w=1$ - even $\\overline{VE}$ is not learnable, but the parameter that optimizes it is: \n",
    "\n",
    "<img src=\"images/11TheBellmanErrorIsNotLearnable2.png\"/>\n",
    "\n",
    "With Bellman Error, the situation is different - it is not learnable, not even in parameters:\n",
    "<img src=\"images/11TheBellmanErrorIsNotLearnable3.png\"/>\n",
    "\n",
    "Example - generates the same data, but results in different minimizers\n",
    "<img src=\"images/11TheBellmanErrorIsNotLearnable4.png\"/>\n",
    "\n",
    "Asssuming that $B$ and $B'$ cannot be distinguished. For the first $w=(0,0)$, for the second (complicated calculation), it is $w=(-\\frac{1}{2},0)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-TD Methods\n",
    "Minimization of $\\overline{PBE}$. It is a true gradient descent method.\n",
    "\n",
    "$$\n",
    "\\nabla\\overline{PBE}(w) = 2\\mathbb{E}[\\rho(\\gamma x_{t+1}-x_t)x_t)x_t^{\\intercal}]\\mathbb{E}[x_t x_t^{\\intercal}]^{-1}\\mathbb{E}[\\rho_t\\delta_t x_t]\n",
    "$$\n",
    "\n",
    "There is a trick how to \n",
    "\n",
    "- get unbiased estimate of each factor of that product (first and third depend on $x_{t+1}$).\n",
    "- calculate the inversion efficiently\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t + \\alpha \\rho_t(\\delta_t x_t - \\gamma x_{t+1}x_t^\\intercal v_t)\n",
    "$$\n",
    "with\n",
    "$$\n",
    "v_{t+1}=v_t+\\beta\\rho_t(\\delta_t-v_t^{\\intercal}x_t)x_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emphatic-TD Methods\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta_t = R_{t+1}+\\gamma \\hat v (S_{t+1},w_t) - \\hat v (S_t,w_t) \n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t + \\alpha M_t \\rho_t \\delta_t \\nabla \\hat v (S_t,w_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "M_t = \\gamma \\rho_{t-1}M_{t-1}+I_t\n",
    "$$\n",
    "with $I_t$ is interest and $M_t$ is emphasis, initiated $M_0=0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Variance\n",
    "\n",
    "Off-policy => logically increased variance\n",
    "\n",
    "Tricks to cope with that considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
